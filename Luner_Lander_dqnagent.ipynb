{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b704a64c-c002-43b7-bddb-d20b449a63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mac\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-297.93 +/- 157.79\n",
      "Episode length: 67.20 +/- 12.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-300.04 +/- 41.58\n",
      "Episode length: 60.20 +/- 8.61\n",
      "Eval num_timesteps=3000, episode_reward=-348.81 +/- 154.88\n",
      "Episode length: 70.80 +/- 14.41\n",
      "Eval num_timesteps=4000, episode_reward=-355.92 +/- 118.54\n",
      "Episode length: 72.20 +/- 12.20\n",
      "Eval num_timesteps=5000, episode_reward=-337.22 +/- 101.07\n",
      "Episode length: 72.60 +/- 11.20\n",
      "Eval num_timesteps=6000, episode_reward=-341.74 +/- 82.41\n",
      "Episode length: 69.20 +/- 14.29\n",
      "Eval num_timesteps=7000, episode_reward=-383.23 +/- 77.85\n",
      "Episode length: 71.00 +/- 9.67\n",
      "Eval num_timesteps=8000, episode_reward=-300.07 +/- 166.84\n",
      "Episode length: 68.60 +/- 11.29\n",
      "Eval num_timesteps=9000, episode_reward=-313.14 +/- 130.48\n",
      "Episode length: 69.00 +/- 14.46\n",
      "Eval num_timesteps=10000, episode_reward=-368.66 +/- 52.64\n",
      "Episode length: 68.40 +/- 9.00\n",
      "Eval num_timesteps=11000, episode_reward=-282.26 +/- 115.95\n",
      "Episode length: 63.80 +/- 7.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-269.73 +/- 41.74\n",
      "Episode length: 67.00 +/- 6.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-390.58 +/- 100.18\n",
      "Episode length: 71.80 +/- 10.80\n",
      "Eval num_timesteps=14000, episode_reward=-365.99 +/- 99.68\n",
      "Episode length: 71.80 +/- 10.01\n",
      "Eval num_timesteps=15000, episode_reward=-313.91 +/- 90.25\n",
      "Episode length: 65.80 +/- 9.56\n",
      "Eval num_timesteps=16000, episode_reward=-337.10 +/- 81.86\n",
      "Episode length: 68.40 +/- 5.39\n",
      "Eval num_timesteps=17000, episode_reward=-291.95 +/- 62.62\n",
      "Episode length: 66.00 +/- 9.38\n",
      "Eval num_timesteps=18000, episode_reward=-234.74 +/- 155.73\n",
      "Episode length: 67.00 +/- 8.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=19000, episode_reward=-385.99 +/- 81.62\n",
      "Episode length: 72.60 +/- 12.47\n",
      "Eval num_timesteps=20000, episode_reward=-316.27 +/- 128.87\n",
      "Episode length: 68.00 +/- 9.78\n",
      "Eval num_timesteps=21000, episode_reward=-360.92 +/- 106.72\n",
      "Episode length: 68.80 +/- 6.34\n",
      "Eval num_timesteps=22000, episode_reward=-447.03 +/- 100.11\n",
      "Episode length: 78.40 +/- 8.73\n",
      "Eval num_timesteps=23000, episode_reward=-359.11 +/- 78.55\n",
      "Episode length: 72.60 +/- 3.56\n",
      "Eval num_timesteps=24000, episode_reward=-369.42 +/- 81.72\n",
      "Episode length: 67.20 +/- 8.49\n",
      "Eval num_timesteps=25000, episode_reward=-260.13 +/- 103.82\n",
      "Episode length: 66.00 +/- 12.17\n",
      "Eval num_timesteps=26000, episode_reward=-279.19 +/- 82.45\n",
      "Episode length: 68.00 +/- 10.35\n",
      "Eval num_timesteps=27000, episode_reward=-298.84 +/- 100.55\n",
      "Episode length: 70.60 +/- 5.08\n",
      "Eval num_timesteps=28000, episode_reward=-276.69 +/- 95.03\n",
      "Episode length: 63.20 +/- 9.68\n",
      "Eval num_timesteps=29000, episode_reward=-339.11 +/- 85.27\n",
      "Episode length: 74.20 +/- 6.18\n",
      "Eval num_timesteps=30000, episode_reward=-393.21 +/- 115.75\n",
      "Episode length: 74.80 +/- 7.08\n",
      "Eval num_timesteps=31000, episode_reward=-273.49 +/- 73.62\n",
      "Episode length: 57.00 +/- 3.63\n",
      "Eval num_timesteps=32000, episode_reward=-298.09 +/- 90.91\n",
      "Episode length: 65.20 +/- 7.57\n",
      "Eval num_timesteps=33000, episode_reward=-275.55 +/- 142.62\n",
      "Episode length: 66.40 +/- 6.77\n",
      "Eval num_timesteps=34000, episode_reward=-284.06 +/- 115.25\n",
      "Episode length: 68.60 +/- 12.94\n",
      "Eval num_timesteps=35000, episode_reward=-380.01 +/- 146.41\n",
      "Episode length: 70.40 +/- 14.54\n",
      "Eval num_timesteps=36000, episode_reward=-319.65 +/- 96.89\n",
      "Episode length: 65.60 +/- 7.99\n",
      "Eval num_timesteps=37000, episode_reward=-369.67 +/- 94.10\n",
      "Episode length: 70.00 +/- 11.03\n",
      "Eval num_timesteps=38000, episode_reward=-200.12 +/- 101.86\n",
      "Episode length: 66.20 +/- 9.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=39000, episode_reward=-396.90 +/- 58.08\n",
      "Episode length: 74.00 +/- 9.88\n",
      "Eval num_timesteps=40000, episode_reward=-288.48 +/- 63.51\n",
      "Episode length: 68.40 +/- 6.09\n",
      "Eval num_timesteps=41000, episode_reward=-415.34 +/- 98.79\n",
      "Episode length: 76.20 +/- 9.33\n",
      "Eval num_timesteps=42000, episode_reward=-337.46 +/- 94.97\n",
      "Episode length: 69.20 +/- 9.33\n",
      "Eval num_timesteps=43000, episode_reward=-299.89 +/- 102.94\n",
      "Episode length: 69.60 +/- 13.03\n",
      "Eval num_timesteps=44000, episode_reward=-283.56 +/- 72.67\n",
      "Episode length: 64.00 +/- 11.58\n",
      "Eval num_timesteps=45000, episode_reward=-299.21 +/- 118.98\n",
      "Episode length: 63.20 +/- 11.91\n",
      "Eval num_timesteps=46000, episode_reward=-334.28 +/- 91.72\n",
      "Episode length: 67.20 +/- 12.46\n",
      "Eval num_timesteps=47000, episode_reward=-380.01 +/- 121.12\n",
      "Episode length: 73.40 +/- 10.56\n",
      "Eval num_timesteps=48000, episode_reward=-260.12 +/- 89.60\n",
      "Episode length: 58.40 +/- 8.96\n",
      "Eval num_timesteps=49000, episode_reward=-306.86 +/- 149.92\n",
      "Episode length: 73.20 +/- 7.36\n",
      "Eval num_timesteps=50000, episode_reward=-209.93 +/- 136.26\n",
      "Episode length: 62.80 +/- 6.91\n",
      "Eval num_timesteps=51000, episode_reward=-238.13 +/- 17.40\n",
      "Episode length: 838.80 +/- 322.40\n",
      "Eval num_timesteps=52000, episode_reward=-224.34 +/- 61.04\n",
      "Episode length: 863.20 +/- 273.60\n",
      "Eval num_timesteps=53000, episode_reward=-245.78 +/- 154.14\n",
      "Episode length: 862.80 +/- 274.40\n",
      "Eval num_timesteps=54000, episode_reward=-116.21 +/- 18.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-141.55 +/- 34.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=56000, episode_reward=-123.44 +/- 30.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=57000, episode_reward=-205.37 +/- 134.21\n",
      "Episode length: 880.00 +/- 240.00\n",
      "Eval num_timesteps=58000, episode_reward=-116.45 +/- 35.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=59000, episode_reward=-357.24 +/- 314.36\n",
      "Episode length: 723.00 +/- 339.26\n",
      "Eval num_timesteps=60000, episode_reward=-194.54 +/- 84.30\n",
      "Episode length: 799.80 +/- 245.23\n",
      "Eval num_timesteps=61000, episode_reward=-154.28 +/- 114.59\n",
      "Episode length: 889.00 +/- 222.00\n",
      "Eval num_timesteps=62000, episode_reward=-177.69 +/- 95.86\n",
      "Episode length: 796.00 +/- 250.77\n",
      "Eval num_timesteps=63000, episode_reward=-293.99 +/- 159.12\n",
      "Episode length: 771.80 +/- 280.20\n",
      "Eval num_timesteps=64000, episode_reward=-91.18 +/- 74.24\n",
      "Episode length: 599.80 +/- 330.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=12.20 +/- 149.48\n",
      "Episode length: 775.20 +/- 183.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=66000, episode_reward=-115.36 +/- 54.33\n",
      "Episode length: 774.20 +/- 277.05\n",
      "Eval num_timesteps=67000, episode_reward=-65.92 +/- 158.83\n",
      "Episode length: 661.60 +/- 278.38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5724\\3902582383.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Train the agent with the evaluation callback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Load the training results from the callback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     ) -> SelfDQN:\n\u001b[1;32m--> 269\u001b[1;33m         return super().learn(\n\u001b[0m\u001b[0;32m    270\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             rollout = self.collect_rollouts(\n\u001b[0m\u001b[0;32m    312\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mtrain_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;31m# Rescale and perform action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m             \u001b[0mnew_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \"\"\"\n\u001b[0;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"terminal_observation\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_from_buf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36m_save_obs\u001b[1;34m(self, env_idx, obs)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_obs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuf_obs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "import gym\n",
    "\n",
    "# Create the Lunar Lander environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Define the DQN agent with specified parameters\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.001,\n",
    "    buffer_size=50000,\n",
    "    batch_size=64,\n",
    "    learning_starts=50000,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "# Create a callback for evaluation and TensorBoard logging\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env=env,\n",
    "   best_model_save_path = \"C:\\\\Users\\\\Mac\\\\Desktop\\\\sems 10\\\\RL\\\\logs\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=1000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "# Train the agent with the evaluation callback\n",
    "model.learn(total_timesteps=250000, callback=eval_callback)\n",
    "\n",
    "# Load the training results from the callback\n",
    "x, y = ts2xy(load_results(\"./logs/\"), \"timesteps\")\n",
    "\n",
    "# Print the mean reward values\n",
    "print(\"Mean rewards:\")\n",
    "for t, reward in zip(x, y):\n",
    "    print(f\"Timestep: {t}, Mean Reward: {reward}\")\n",
    "\n",
    "# Close the environments\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9c42a-fe2c-4b5d-8072-0792c98e07d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
